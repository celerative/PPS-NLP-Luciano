{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASIFICACION DE TEXTO\n",
    "** En este notebook se realizarán algunas pruebas de clasificación de texto utilizando el dataset de Spooky Authors**\n",
    "\n",
    "Los modelos utilizados serán:\n",
    "\n",
    "   ** -Logistic Regression**\n",
    "   \n",
    "   ** -Naive Bayes**\n",
    "   \n",
    "   ** -Singular Value Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from collections import Counter\n",
    "from scipy.misc import imread\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk import word_tokenize\n",
    "import gensim \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agrego lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "class LemmaTFidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(LemmaTFidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementamos una métrica llamada logloss multiclase para evaluar las predicciones de los modelos a utilizar** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defino una version de la metrica de Logarithmic Loss, que funcione para múltiples clases\n",
    "\n",
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo datos \n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'EAP' 'MWS' 'HPL' 'MWS' 'EAP' 'EAP' 'EAP' 'MWS']\n",
      "[0 1 0 2 1 2 0 0 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Label encoding de los valores de autor\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "print(train.author.values[:10])\n",
    "\n",
    "y = lbl_enc.fit_transform(train.author.values)\n",
    "\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her hair was the brightest living gold, and despite the poverty of her clothing, seemed to set a crown of distinction on her head.'\n",
      " '\"No,\" he said, \"oh, no a member of my family my niece, and a most accomplished woman.\"']\n",
      "[2 0]\n"
     ]
    }
   ],
   "source": [
    "#Separacion de los datos en entrenamiento y validacion\n",
    "# Stratify implica que la proporcion de valores que va a haber al dividir los datos, va a ser igual \n",
    "#Acá le paso y, asi que si hay 30% valor 0, 20 %valor 1 y 50% valor 2, la misma proporcion va a estar en los datos de entr y test\n",
    "#Random state es la seed para el random number generator\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "print(xtrain[:2])\n",
    "print(ytrain[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento: 17621\n",
      "\n",
      "Datos de validación: 1958\n"
     ]
    }
   ],
   "source": [
    "print (\"Datos de entrenamiento:\",len(xtrain))\n",
    "print (\"\\nDatos de validación:\",len(xvalid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621, 8089)\n",
      "\n",
      " 17621 frases y vocabulario de 8881 palabras unicas \n",
      "\n",
      "  (0, 6341)\t0.2469424385443994\n",
      "  (0, 5395)\t0.31534173991151543\n",
      "  (0, 4206)\t0.2621357570531394\n",
      "  (0, 3359)\t0.21913481148346806\n",
      "  (0, 3286)\t0.2699803359499066\n",
      "  (0, 3163)\t0.28210670387497144\n",
      "  (0, 2094)\t0.33928338965309934\n",
      "  (0, 1927)\t0.2913532806410929\n",
      "  (0, 1657)\t0.3345950093082599\n",
      "  (0, 1267)\t0.3474674493892856\n",
      "  (0, 913)\t0.3710563608683541\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "\n",
    "#\n",
    "#tfvec = TfidfVectorizer(min_df=5,  max_features=None, \n",
    "#            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "#            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "#            stop_words = 'english')\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.85, \n",
    "                                     min_df=5,\n",
    "                                     stop_words='english',\n",
    "                                     decode_error='ignore')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets\n",
    "tf_vectorizer.fit(list(xtrain) + list(xvalid)) \n",
    "#tf_vectorizer.fit(list(xtrain))\n",
    "xtrain_tfvec =  tf_vectorizer.transform(xtrain) \n",
    "xvalid_tfvec = tf_vectorizer.transform(xvalid)\n",
    "print(xtrain_tfvec.shape)\n",
    "print(\"\\n 17621 frases y vocabulario de 8881 palabras unicas \\n\")\n",
    "print(xtrain_tfvec[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.791\n",
      "logloss: 0.622 \n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on TFIDF\n",
    "clf1 = LogisticRegression(C=1.0)\n",
    "clf1.fit(xtrain_tfvec, ytrain)\n",
    "predictions = clf1.predict_proba(xvalid_tfvec)\n",
    "print(\"Accuracy: %0.3f\" %clf1.score(xvalid_tfvec,yvalid)) #Accuracy \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Vectorizer\n",
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.781\n",
      "logloss: 0.528 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf2 = LogisticRegression(C=1.0)\n",
    "clf2.fit(xtrain_ctv, ytrain)\n",
    "#print(xtrain_ctv)\n",
    "#yvalid\n",
    "print(\"Accuracy: %0.3f\" %clf2.score(xvalid_ctv,yvalid)) #Accuracy \n",
    "predictions = clf2.predict_proba(xvalid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.832\n",
      "logloss: 0.485 \n"
     ]
    }
   ],
   "source": [
    "# simple Naive Bayes on Counts\n",
    "clf3 = MultinomialNB()\n",
    "clf3.fit(xtrain_ctv, ytrain)\n",
    "print(\"score: %0.3f\"% clf3.score(xvalid_ctv,yvalid))\n",
    "predictions = clf3.predict_proba(xvalid_ctv)\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfvec)\n",
    "xtrain_svd = svd.transform(xtrain_tfvec)\n",
    "xvalid_svd = svd.transform(xvalid_tfvec)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-26df4128eda2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Fitting a simple SVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclf4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# since we need probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_svd_scl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid_svd_scl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(\"score: %0.3f\"% clf4.score(xvalid_ctv,yvalid))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf4 = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf4.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf4.predict_proba(xvalid_svd_scl)\n",
    "#print(\"score: %0.3f\"% clf4.score(xvalid_ctv,yvalid))\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
